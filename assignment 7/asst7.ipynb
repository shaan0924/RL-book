{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CME 241 Assignment 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shaan Patel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the utility function is $U(x) = \\log(x)$. Therefore, we want to find the policy: $(t,W_t) \\rarr (\\pi_t, c_t)$ that maximizes the expected return, aka the value function:\n",
    "\n",
    "$$V^*(t,W_t) = \\max_{\\pi,c} E_t[\\int_t^T e^{-\\rho(s-t)}\\log(c_s) ds + e^{-\\rho(T-t)} \\epsilon^\\gamma \\log(W_T)]$$\n",
    "\n",
    "$$= \\max_{\\pi,c} E_t[\\int_t^{t_1} e^{-\\rho(s-t)}\\log(c_s) ds + e^{-\\rho(t_1-t)} V^*(t_1,W_{t_1})]$$\n",
    "\n",
    "$$e^{-\\rho t} V^*(t,W_t) = \\max_{\\pi,c} E_t[\\int_t^{t_1} e^{-\\rho s}\\log(c_s) ds + e^{-\\rho t_1} V^*(t_1,W_{t_1})] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In stochastic differential form, we get:\n",
    "\n",
    "$$\\max_{\\pi_t, c_t} E[d(e^{-\\rho t} V^*(t,W_t)) + e^{-\\rho t}\\log(c_t) dt] = 0$$\n",
    "\n",
    "$$ \\Rarr \\max_{\\pi_t, c_t} E[dV^*(t,W_t) + \\log(c_t) dt] = \\rho V^*(t,W_t) dt $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Ito's lemma, we get the following:\n",
    "\n",
    "$$\\max_{\\pi_t, c_t} [\\frac{\\partial V^*}{\\partial t} + \\frac{\\partial V^*}{\\partial W_t}((\\pi_t (\\mu - r) + r)W_t - c_t) + \\frac{\\partial^2 V^*}{\\partial W_t^2} \\frac{\\pi_t^2 \\sigma^2 W_t^2}{2} + \\log(c_t)]$$\n",
    "$$ = \\rho V^*(t, W_t) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the optimal $\\pi^*_t$, $c^*_t$, we take the first derivatives with respect to $\\pi_t$ and $c_t$, and equate it to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First derivative with respect to $\\pi_t$:\n",
    "\n",
    "$$ (\\mu - r) \\frac{\\partial V^*}{\\partial W_t} + \\frac{\\partial^2 V^*}{\\partial W_t^2} \\pi_t \\sigma^2 W_t = 0 $$\n",
    "\n",
    "$$ \\Rarr \\pi^*_t = \\frac{-\\frac{\\partial V^*}{\\partial W_t} (\\mu - r)}{\\frac{\\partial^2 V^*}{\\partial W_t^2}\\sigma^2 W_t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First derivative with respect to $c_t$:\n",
    "\n",
    "$$ -\\frac{\\partial V^*}{\\partial W_t} + \\frac{1}{c_t} = 0$$\n",
    "\n",
    "$$ \\Rarr c^*_t = (\\frac{\\partial V^*}{\\partial W_t})^{-1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plugging these optimal values into the original PDE gives us \n",
    "\n",
    "$$ \\frac{\\partial V^*}{\\partial t}  - \\frac{(\\mu - r)^2}{2\\sigma^2}\\frac{(\\frac{\\partial V^*}{\\partial W_t})^2}{\\frac{\\partial^2 V^*}{\\partial W_t^2}} + \\frac{\\partial V^*}{\\partial W_t} r W_t - \\log(\\frac{\\partial V^*}{\\partial W_t}) $$\n",
    "$$ = \\rho V^*(t, W_t) $$\n",
    "\n",
    "With boundary condition:\n",
    "\n",
    "$$ V^*(T, W_T) = U(W_T) = \\log(W_T) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let:\n",
    "\n",
    "$$V^*(t, W_t) = f(t) + \\log(W_t) $$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\\frac{\\partial V^*}{\\partial t} = f'(t) $$\n",
    "$$\\frac{\\partial V^*}{\\partial W_t} = \\frac{1}{W_t} $$\n",
    "$$\\frac{\\partial^2 V^*}{\\partial W_t^2} = \\frac{-1}{W_t^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting this into the PDE, we get:\n",
    "\n",
    "$$ f'(t) + \\frac{(\\mu - r)^2}{2 \\sigma^2} + r - \\log(\\frac{1}{W_t}) = \\rho (f(t) +  \\log(W_t))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simplifies to:\n",
    "\n",
    "$$ f'(t) = \\rho f(t) + \\log(W_t^{\\rho - 1}) - \\frac{(\\mu - r)^2}{2\\sigma^2} - r$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting $v_t$ for $\\log(W_t^{\\rho - 1}) - \\frac{(\\mu - r)^2}{2\\sigma^2} - r$ we get\n",
    "\n",
    "$$f'(t) = \\rho f(t) + v_t $$\n",
    "\n",
    "with boundary condition $f(T) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This differential equation has a solution of the form:\n",
    "\n",
    "$$f(t) = \\frac{v_T}{\\rho e^{\\rho T}}e^{\\rho t} - \\frac{v_t}{\\rho} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leads to our value function becoming\n",
    "\n",
    "$$\\frac{v_T}{\\rho e^{\\rho T}}e^{\\rho t} - \\frac{v_t}{\\rho} + \\log(W_t) $$\n",
    "\n",
    "Thus, our optimal policy is:\n",
    "\n",
    "$$\\pi^*_t = \\frac{-\\frac{\\partial V^*}{\\partial W_t} (\\mu - r)}{\\frac{\\partial^2 V^*}{\\partial W_t^2}\\sigma^2 W_t} $$\n",
    "$$ \\Rarr \\frac{\\mu - r}{\\sigma} $$\n",
    "\n",
    "and our consumption is:\n",
    "\n",
    "$$c^*_t = (\\frac{\\partial V^*}{\\partial W_t})^{-1} = W_t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this situation, our action space is determined by $\\alpha$, aka how much time we decide to spend working or learning.\n",
    "\n",
    "$$A = \\alpha \\in [0,1] $$\n",
    "\n",
    "The states for this scenario is each day, where, if employed, we have a probability of being unemployed, and if unemployed, we have a probability of being re-employed. In addition, we have our current skill level\n",
    "\n",
    "$$S = (\\{u, e\\}, s \\in [0,\\infty) )$$\n",
    "\n",
    "In the event we get fired or remain unemployed, then there is no action, as we cannot work so we spend our day losing skill.\n",
    "\n",
    "In the event that we get a job or remain employed, we have to determine the utility gained from working. Given that the probability of being fired is constant and does not depend on the fraction of time we work, the optimal policy could be to spend the first few days using all your time to build up your skill level. This will cause greater utility because we are not working but also once our skill level has reached a sufficient level, we can then go work to achieve a higher wage. The optimal time choice in this situation will then have to be a function of the expected future reward from earnings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our rewards in each state would be how much utility we gained from consumption and leisure that day.\n",
    "\n",
    "As a result, our transition function would be of the form:\n",
    "\n",
    "$$P(S_{t+1} = (m', s'), R_{t+1} = r|S_t = (m,s), A_t =\\alpha)$$\n",
    "\n",
    "The reward is 0 if the future state is unemployed ($m' = u$). This happens with probability $p$ given that we are currently employed, and $1 - h(s)$ if we are currently unemployed. As a result, there is no optimal policy in this situation.\n",
    "\n",
    "If the future state is employed, the reward depends on the utility of earnings. As a result, this is 0 if we are re-employed as no earnings were made. If we are continuing employment, earnings is a function of wage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$R = \\begin{cases}\n",
    "    0, & m = u \\\\\n",
    "    U(f(s)*\\alpha), & m = e \n",
    "    \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall our transition function would look like:\n",
    "\n",
    "$$P(S_{t+1} = (m', s'), R_{t+1} = r|S_t = (m,s), A_t =\\alpha) = $$\n",
    "$$\n",
    "\\begin{cases}\n",
    "    P(S_{t+1} = (m', s + 60(1-\\alpha)g(s)), R_{t+1} = U(f(s)*\\alpha)), & m = e \\\\\n",
    "    P(S_{t+1} = (m', s(1-\\lambda), R_{t+1} = 0)), & m = u\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "When accounting for employment status this becomes\n",
    "\n",
    "$$P((e,s'), r, (m,s), \\alpha) = \\begin{cases}\n",
    "    1-p, & s' = s + 60(1-\\alpha)g(s), & r = U(f(s)*\\alpha), & m = e \\\\\n",
    "    h(s), & s' = s(1-\\lambda), & r = 0, & m = u\n",
    "    \\end{cases}$$\n",
    "\n",
    "$$P((u,s'), r, (m,s),\\alpha) = \\begin{cases}\n",
    "    p, & s' = s + 60(1-\\alpha)g(s), & r = U(f(s)*\\alpha), & m = e \\\\\n",
    "    1-h(s), & s' = s(1-\\lambda), & r = 0, & m = u\n",
    "    \\end{cases}$$\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "579dcdfb899fc187fdf97538744cc3a387eca9f5084bb8d1591af8d2d48fab3d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
