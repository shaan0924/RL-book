{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CME 241 Assignment 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shaan Patel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the utility function is $U(x) = \\log(x)$. Therefore, we want to find the policy: $(t,W_t) \\rarr (\\pi_t, c_t)$ that maximizes the expected return, aka the value function:\n",
    "\n",
    "$$V^*(t,W_t) = \\max_{\\pi,c} E_t[\\int_t^T e^{-\\rho(s-t)}\\log(c_s) ds + e^{-\\rho(T-t)} \\epsilon^\\gamma \\log(W_T)]$$\n",
    "\n",
    "$$= \\max_{\\pi,c} E_t[\\int_t^{t_1} e^{-\\rho(s-t)}\\log(c_s) ds + e^{-\\rho(t_1-t)} V^*(t_1,W_{t_1})]$$\n",
    "\n",
    "$$e^{-\\rho t} V^*(t,W_t) = \\max_{\\pi,c} E_t[\\int_t^{t_1} e^{-\\rho s}\\log(c_s) ds + e^{-\\rho t_1} V^*(t_1,W_{t_1})] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In stochastic differential form, we get:\n",
    "\n",
    "$$\\max_{\\pi_t, c_t} E[d(e^{-\\rho t} V^*(t,W_t)) + e^{-\\rho t}\\log(c_t) dt] = 0$$\n",
    "\n",
    "$$ \\Rarr \\max_{\\pi_t, c_t} E[dV^*(t,W_t) + \\log(c_t) dt] = \\rho V^*(t,W_t) dt $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Ito's lemma, we get the following:\n",
    "\n",
    "$$\\max_{\\pi_t, c_t} [\\frac{\\partial V^*}{\\partial t} + \\frac{\\partial V^*}{\\partial W_t}((\\pi_t (\\mu - r) + r)W_t - c_t) + \\frac{\\partial^2 V^*}{\\partial W_t^2} \\frac{\\pi_t^2 \\sigma^2 W_t^2}{2} + \\log(c_t)]$$\n",
    "$$ = \\rho V^*(t, W_t) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the optimal $\\pi^*_t$, $c^*_t$, we take the first derivatives with respect to $\\pi_t$ and $c_t$, and equate it to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First derivative with respect to $\\pi_t$:\n",
    "\n",
    "$$ (\\mu - r) \\frac{\\partial V^*}{\\partial W_t} + \\frac{\\partial^2 V^*}{\\partial W_t^2} \\pi_t \\sigma^2 W_t = 0 $$\n",
    "\n",
    "$$ \\Rarr \\pi^*_t = \\frac{-\\frac{\\partial V^*}{\\partial W_t} (\\mu - r)}{\\frac{\\partial^2 V^*}{\\partial W_t^2}\\sigma^2 W_t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First derivative with respect to $c_t$:\n",
    "\n",
    "$$ -\\frac{\\partial V^*}{\\partial W_t} + \\frac{1}{c_t} = 0$$\n",
    "\n",
    "$$ \\Rarr c^*_t = (\\frac{\\partial V^*}{\\partial W_t})^{-1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plugging these optimal values into the original PDE gives us \n",
    "\n",
    "$$ \\frac{\\partial V^*}{\\partial t}  - \\frac{(\\mu - r)^2}{2\\sigma^2}\\frac{(\\frac{\\partial V^*}{\\partial W_t})^2}{\\frac{\\partial^2 V^*}{\\partial W_t^2}} + \\frac{\\partial V^*}{\\partial W_t} r W_t - \\log(\\frac{\\partial V^*}{\\partial W_t}) $$\n",
    "$$ = \\rho V^*(t, W_t) $$\n",
    "\n",
    "With boundary condition:\n",
    "\n",
    "$$ V^*(T, W_T) = \\epsilon^\\gamma \\log(W_T) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let:\n",
    "\n",
    "$$V^*(t, W_t) = f(t)^\\gamma \\log(W_t) $$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\\frac{\\partial V^*}{\\partial t} = \\gamma f(t)^{\\gamma - 1} f'(t) \\log(W_t) $$\n",
    "$$\\frac{\\partial V^*}{\\partial W_t} = \\frac{f(t)^\\gamma }{W_t} $$\n",
    "$$\\frac{\\partial^2 V^*}{\\partial W_t^2} = \\frac{-f(t)^\\gamma}{W_t^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting this into the PDE, we get:\n",
    "\n",
    "$$ \\gamma f(t)^{\\gamma - 1} f'(t) \\log(W_t) + \\frac{(\\mu - r)^2}{2 \\sigma^2} f(t)^\\gamma + r f(t)^\\gamma - \\log(\\frac{f(t)^\\gamma}{W_t}) = \\rho f(t)^\\gamma \\log(W_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\gamma$ is 1 in this situation when $\\log$ is used, so this simplifies to:\n",
    "\n",
    "$$ f'(t) \\log(W_t) + \\frac{(\\mu - r)^2}{2\\sigma^2}f(t) + rf(t) - \\log(\\frac{f(t)}{W_t}) = \\rho f(t) \\log(W_t) $$\n",
    "\n",
    "$$ \\Rarr f'(t) \\log(W_t) + \\frac{(\\mu - r)^2}{2\\sigma^2}f(t) + rf(t) - \\log(f(t)) + \\log(W_t) = \\rho f(t) \\log(W_t) $$\n",
    "\n",
    "$$ \\Rarr f'(t) + f(t)[\\frac{(\\mu - r)^2}{2\\sigma^2 \\log(W_t)} + \\frac{r}{\\log(W_t)} - \\rho \\log(W_t)] - \\frac{\\log(f(t))}{\\log(W_t)} + 1 = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting $v$ for $\\frac{(\\mu - r)^2}{2\\sigma^2 \\log(W_t)} + \\frac{r}{\\log(W_t)} - \\rho \\log(W_t)$ we get\n",
    "\n",
    "$$f'(t) = \\frac{\\log(f(t))}{\\log(W_t)} - 1 - vf(t) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this situation, our action space is determined by $\\alpha$, aka how much time we decide to spend working or learning.\n",
    "\n",
    "$$A = \\alpha \\in [0,1] $$\n",
    "\n",
    "The states for this scenario is each day, where, if employed, we have a probability of being unemployed, and if unemployed, we have a probability of being re-employed. \n",
    "\n",
    "$$S = \\{u, e\\}$$\n",
    "\n",
    "In each state, we would first determine hiring status. In the event we get fired or remain unemployed, then there is no action, as we cannot work so we spend our day losing skill.\n",
    "\n",
    "In the event that we get a job or remain employed, we have to determine the utility gained from learning and working. We can assume that there is utility gained from not working and there is also utility gained from consumption. Given that the probability of being fired is constant and does not depend on the fraction of time we work, the optimal policy could be to spend the first few days spend all your time learning to build up your skill level. This will cause greater utility because we are not working but also once our skill level has reached a sufficient level, we can then go work to achieve a higher wage. The optimal time choice in this situation will then have to be a function of the expected future reward from consumption and leisure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our rewards in each state would be how much utility we gained from consumption and leisure that day.\n",
    "\n",
    "As a result, our transition function would be of the form:\n",
    "\n",
    "$$P(S_{t+1} = m', R_{t+1} = r|S_t = m, A_t =\\alpha)$$\n",
    "\n",
    "The reward is 0 if the future state is unemployed ($m' = u$). This happens with probability $p$ given that we are currently employed, and $1 - h(s)$ if we are currently unemployed. As a result, there is no optimal policy in this situation.\n",
    "\n",
    "If the future state is employed, the reward depends on the utility of consumption and leisure. As a result, this is 0 if we are re-employed as no leisure or consumption was done. If we are continuing employment, consumption is a function of wage/wealth while leisure is a function of $1- \\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$R = \\begin{cases}\n",
    "    0, & m = u \\\\\n",
    "    U(f(s)*\\alpha) + U(1-\\alpha), & m = e \n",
    "    \\end{cases}$$"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "579dcdfb899fc187fdf97538744cc3a387eca9f5084bb8d1591af8d2d48fab3d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
