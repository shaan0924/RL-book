{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CME 241 Assignment 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shaan Patel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Iterator, TypeVar, Tuple, Sequence, Callable, Dict\n",
    "import rl.markov_decision_process as mp\n",
    "import rl.distribution as ds\n",
    "from rl.policy import DeterministicPolicy\n",
    "from rl.approximate_dynamic_programming import QValueFunctionApprox\n",
    "from rl.function_approx import LinearFunctionApprox, Weights\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = TypeVar('S')\n",
    "A = TypeVar('A')\n",
    "\n",
    "def greedy_policy_from_qvf(\n",
    "    q: QValueFunctionApprox[S,A],\n",
    "    actions: Callable[[mp.NonTerminal[S]], Iterable[A]]\n",
    ") -> DeterministicPolicy[S,A]:\n",
    "    def optimal_action(s: S) -> A:\n",
    "        _, a = q.argmax((mp.NonTerminal(s), a) for a in actions(mp.NonTerminal(s)))\n",
    "        return a\n",
    "    return DeterministicPolicy(optimal_action)\n",
    "\n",
    "\n",
    "def LSTD(\n",
    "    transitions: Iterable[mp.TransitionStep[S,A]],\n",
    "    feature_functions: Sequence[Callable[[mp.NonTerminal[S],A],float]],\n",
    "    approx_pol: DeterministicPolicy[mp.NonTerminal[S],A],\n",
    "    gamma: float,\n",
    "    epsilon: float\n",
    ") -> LinearFunctionApprox[Tuple[mp.NonTerminal[S],A]]:\n",
    "\n",
    "    num_features: int = len(feature_functions)\n",
    "    a_inv: np.ndarray = np.eye(num_features) / epsilon  \n",
    "    b_vec: np.ndarray = np.zeros(num_features)\n",
    "\n",
    "    pi: DeterministicPolicy[S,A] = approx_pol\n",
    "\n",
    "    for tr in transitions:\n",
    "        phi1: np.ndarray = np.array([f(tr.state,tr.action) for f in feature_functions])\n",
    "        if isinstance(tr.next_state, mp.NonTerminal):\n",
    "            phi2 = phi1 - gamma * np.array([f((tr.next_state, pi.action_for(tr.next_state.state))) for f in feature_functions])\n",
    "        else:\n",
    "            phi2 = phi1\n",
    "        temp: np.ndarray = a_inv.T.dot(phi2)\n",
    "        a_inv = a_inv - np.outer(a_inv.dot(phi1), temp) / (1 + phi1.dot(temp))\n",
    "        b_vec += phi1 * tr.reward\n",
    "    opt_wts: np.ndarray = a_inv.dot(b_vec)\n",
    "    return LinearFunctionApprox.create(\n",
    "        feature_functions=feature_functions,\n",
    "        weights=Weights.create(opt_wts)\n",
    "    )\n",
    "\n",
    "def LSPI(\n",
    "    transitions: Iterable[mp.TransitionStep[S,A]],\n",
    "    actions: Callable[[mp.NonTerminal[S]], Iterable[A]],\n",
    "    feature_functions: Sequence[Callable[[mp.NonTerminal[S],A],float]],\n",
    "    approx_pol: DeterministicPolicy[mp.NonTerminal[S],A],\n",
    "    gamma: float,\n",
    "    epsilon: float\n",
    ") -> Iterator[LinearFunctionApprox[Tuple[mp.NonTerminal[S], A]]]:\n",
    "    pi: DeterministicPolicy[S,A] = approx_pol\n",
    "    transition_seq: Sequence[mp.TransitionStep[S,A]] = list(transitions)\n",
    "    while True:\n",
    "        q: LinearFunctionApprox[Tuple[mp.NonTerminal[S], A]] =\\\n",
    "            LSTD(\n",
    "                transitions=transition_seq,\n",
    "                feature_functions=feature_functions,\n",
    "                approx_pol=pi,\n",
    "                gamma=gamma,\n",
    "                epsilon=epsilon\n",
    "            )\n",
    "        pi = greedy_policy_from_qvf(q, actions)\n",
    "        yield q\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptionsMDP(mp.MarkovDecisionProcess[int, int]):\n",
    "    price: float\n",
    "    strike: float\n",
    "    iscall: bool\n",
    "\n",
    "    def get_payoff(self):\n",
    "        price = self.price\n",
    "        strike = self.strike\n",
    "        iscall = self.iscall\n",
    "\n",
    "        if iscall:\n",
    "            return max(0, price - strike)\n",
    "        else:\n",
    "            return max(0, strike - price)\n",
    "\n",
    "    def actions(self, state: mp.NonTerminal[int]) -> Iterable[str]:\n",
    "        return ['e','c']\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        state: mp.NonTerminal[int],\n",
    "        action: A,\n",
    "    ) -> ds.Distribution[Tuple[mp.State[int], float]]:\n",
    "\n",
    "        if action == 'e':\n",
    "            return ds.Constant(Tuple[mp.State[state + 1], self.get_payoff()])\n",
    "        else:\n",
    "            self.price += np.random.randn()\n",
    "            return ds.Constant(Tuple[mp.State[state + 1], 0])\n",
    "\n",
    "            \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "579dcdfb899fc187fdf97538744cc3a387eca9f5084bb8d1591af8d2d48fab3d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
