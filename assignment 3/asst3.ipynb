{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CME 241 Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shaan Patel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using a deterministic policy, the action $a$ is given by a fixed function $\\pi (s)$ that depends on the state. In this situation, there is no randomness in the action decision, as the state determines what action will be taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As such, the 4 MDP Bellman Policy equations become the following:\n",
    "\n",
    "$ V^{\\pi_D}(s) = R(s, \\pi_D(s)) + \\gamma \\sum_{s' \\in N} P(s, \\pi_D(s), s') V^{\\pi_D}(s') $\n",
    "\n",
    "$ V^{\\pi_D}(s) = Q^{\\pi_D}(s, \\pi_D(s))$\n",
    "\n",
    "$Q^{\\pi_D}(s, \\pi_D(s)) = R(s, \\pi_D(s)) + \\gamma \\sum_{s' \\in N} P(s, \\pi_D(s), s') V^{\\pi_D}(s') $\n",
    "\n",
    "$Q^{\\pi_D}(s, \\pi_D(s)) = R(s, \\pi_D(s)) + \\gamma \\sum_{s' \\in N}P(s, \\pi(s), s') Q^{\\pi_D}(s', \\pi_D(s')) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, the $ \\sum_{a \\in A}$ gets removed because for each $s$ there is a fixed action that is taken based on $\\pi(s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the rewards, transition probabilities, and states do not depend on the explicit value of $s$. As a result, we can see that the explicit value of $s$ is irrelevant to the value function, thus implying that the value function is the same for all values of $s$. Thus, $V^*(s) = V^*(s + 1)$, and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V^*(s) = \\max_{a \\in A} [R(s,a) + \\gamma V^*(s) \\sum_{s' \\in N} P(s,a,s')] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum over all states of $P(s,a,s')$ is 1, so we get\n",
    "\n",
    "$$V^*(s) = \\max_{a \\in A} [R(s,a) + \\gamma V^*(s)] $$\n",
    "\n",
    "This causes $ \\gamma V^*(s)$ to not depend on $a$, so it can be removed from the max equation.\n",
    "\n",
    "$$ (1 - \\gamma)V^*(s) = \\max_{a \\in A} R(s,a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ R(s,a) = E[r_{t+1}|S_t = s, A_t = a] = (1 - a)a + (1+a)(1-a) $$\n",
    "$$ = (1-a)(2a + 1) = 1 + a - 2a^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of $a$ are either 0 or 1, thus $\\max_{a \\in A} R(s,a) = 1 $, when $a$ is 0. Thus,\n",
    "\n",
    "$$ V^*(s) = \\frac{1}{1 - \\gamma} = 2$$\n",
    "\n",
    "To have an optimal deterministic policy, we should hit the optimal value function for each state. As a result, because the value function is the same for all states, the optimal choice for $a$ remains the same across all $s$. Thus,\n",
    "\n",
    "$$ \\pi^*(s) = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state space of this frog problem is all lilypads from 0 to $n$.\n",
    "The action space is the set $A, B$\n",
    "For the transition function, we have\n",
    "\n",
    "if $s' = s - 1$, \n",
    "$$P(s,a,s') = \\frac{s + 1}{2n} $$\n",
    "\n",
    "if $s' = s + 1$,\n",
    "$$P(s,a,s') = \\frac{n-s + 1}{2n}$$\n",
    "\n",
    "all other cases,\n",
    "$$P(s,a,s') = \\frac{1}{2n} $$\n",
    "\n",
    "In this scenario, the frog has an equal chance of choosing either action, and thus there is a 1/2 chance of choosing either croak. In the case of croak A, the frog can only go one space forward or backward. Thus there is no way to go to any state aside from $i-1$ or $i+1$ given croak A. If the frog croaks B, then there is a $\\frac{1}{n}$ chance it can land one space away from its previous state as well, so these are included in the calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Expected Discounted Sum of Costs can be modeled through the MDP Bellman Policy Equation\n",
    "\n",
    "$$V^\\pi(s) = \\sum_{a \\in A} \\pi(s,a) [R(s,a) + \\gamma \\sum_{s' \\in N} P(s,a,s') V^\\pi(s')] $$\n",
    "\n",
    "In the myopic case, $\\gamma$ is 0, so the equation simplifies to \n",
    "\n",
    "$$V^\\pi(s) = \\sum_{a \\in A} \\pi(s,a) R(s,a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are in the continuous case, so the sum is replaced with an integral, giving us\n",
    "\n",
    "$$V^\\pi(s) = \\int_{A} \\pi(s,a) R(s,a) da$$\n",
    "\n",
    "The reward function $R$ is the expected return in the next period given current state $s$ and action $a$. Thus, because $s'$ follows a $N(s,\\sigma)$ distribution, we can see that\n",
    "\n",
    "$$ R(s,a) = E[e^{as'}] = \\int_{\\infty}^{\\infty} e^{ax} \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{\\frac{-1}{2}(\\frac{x - s}{\\sigma})^2} dx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the exponents, we have $2\\sigma^2 ax - x^2 + 2xs - s^2 $ so we can complete the square to get $$(-x^2 + 2x(s + \\sigma^2 a) - (s + \\sigma^2 a)^2) + 2\\sigma^2 as +  \\sigma^4 a^2 $$ or $$-(x - s - \\sigma^2 a)^2 + 2\\sigma^2 as + \\sigma^4 a^2  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the latter two terms have no x, we can take them out of the integral\n",
    "\n",
    "$$ e^{\\frac{\\sigma^2 a (2s + \\sigma^2 a)}{2\\sigma^2}} \\int_\\infty^\\infty \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{\\frac{-1}{2} (\\frac{x - s - \\sigma^2 a}{\\sigma})^2} dx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The integral thus simplifies to the pdf of a $N(s + \\sigma^2 a, \\sigma)$ distribution, so integrating it over all real values equates it to 1. So,\n",
    "\n",
    "$$R(s,a) = e^{\\frac{a (2s + \\sigma^2 a)}{2}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to the value function we see that\n",
    "\n",
    "$$V^\\pi(s) = \\int_A \\pi(s,a) e^{\\frac{a (2s + \\sigma^2 a)}{2}} da$$\n",
    "\n",
    "For any s, the minimum value of this function (provided that $\\pi(s,a)$ is 1 at this minimum value and 0 everywhere else) is when $a(2s + \\sigma^2 a)$ is at its minimum. Therefore, the minimum is reached when $2s + 2\\sigma^2 a= 0$, or\n",
    "\n",
    "$$ a = \\frac{-s}{\\sigma^2} $$\n",
    "\n",
    "This provides a cost of $e^{\\frac{-ss'}{\\sigma^2}}$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
