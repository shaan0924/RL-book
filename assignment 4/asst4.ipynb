{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CME 241 Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shaan Patel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given we know $v_0$, we can now find $v_1$ through the value iteration method.\n",
    "\n",
    "$$ v_1(s) = B^*(v_0)(s) = \\max_{a \\in A} Q^\\pi(s,a)$$\n",
    "\n",
    "$$ = \\max_{a \\in A} [R(s,a) + \\gamma \\sum_{s' \\in N} P(s,a,s') V^*(s')] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for $s_1$ we get\n",
    "\n",
    "$$ \\max_{a \\in A} [R(s_1,a) + \\gamma \\sum_{s' \\in N} P(s_1,a,s')v^*(s')] $$\n",
    "$$ = 8 + (0.2 * 10 + 0.6*1 + 0.2*0) = 10.6$$ \n",
    "for $a_1$ and\n",
    "$$ = 10 + (0.1*10 + 0.2*1 + 0.7*0) = 11.2$$ \n",
    "\n",
    "for $a_2$. So $v_1(s_1) = 11.2$. For $s_2$ we have\n",
    "\n",
    "$$ 1 + (0.3*10 + 0.3*1 + 0.4*0) = 4.3$$ \n",
    "for $a_1$ and\n",
    "$$ -1 + (0.5*10 + 0.3*1 + 0.2*0) = 4.3$$ \n",
    "for $a_2$. So $v_1(s_2) = 4.3$. $v_1(s_3) = 0$ because it is the terminal state and has no rewards.\n",
    "\n",
    "The optimal policy $\\pi_1(s)$ is 2 for $s_1$ and either 1 or 2 for $s_2$, so $\\pi_1(s) = 2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $v_2$ we do a similar step. In the $s_1$ case,\n",
    "\n",
    "$$v_2(s_1) = 8 + (0.2*11.2 + 0.6*4.3 + 0) = 12.82 $$\n",
    "for $a_1$ and\n",
    "$$ = 10 + (0.1*11.2 + 0.2*4.3 + 0) = 11.98 $$\n",
    "\n",
    "for $a_2$. So $v_1(s_1) = 12.82$ for action $a_1$. In the $s_2$ case,\n",
    "\n",
    "$$v_2(s_2) = 1 + (0.3*11.2 + 0.3*4.3 + 0) = 5.65$$\n",
    "for $a_1$ and\n",
    "$$ -1 + (0.5*11.2 + 0.3*4.3 + 0) = 5.89 $$\n",
    "for $a_2$. so $v_1(s_2) = 5.89$ for action $a_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the value functions for each state are increasing with each iteration. As such, the action that has the higher probability of reaching state 1 will yield the higher value function. In the case of $s_1$, we see that action $a_1$ has a higher chance of returning both $s_1$ and $s_2$ over $a_2$. As such, through continuous iterations, action 1 will continuously give higher values than action 2.\n",
    "\n",
    "For $s_2$, we see that the probability of reaching state 2 is the same for both actions, but the chance of reaching state 1 is higher with action 2. Thus, over continuous iterations, this will yield higher sums than in action 1.\n",
    "\n",
    "As a result, $\\pi_k(s)$ for $k > 2$ is equal to $\\pi_2(s)$, which is $a_1$ for $s_1$ and $a_2$ for $s_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict, Mapping\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "from rl.policy import FiniteDeterministicPolicy\n",
    "from rl.markov_process import FiniteMarkovProcess, FiniteMarkovRewardProcess\n",
    "from rl.distribution import Categorical\n",
    "from scipy.stats import poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class InventoryState:\n",
    "    on_hand: int\n",
    "    on_order: int\n",
    "\n",
    "    def inventory_position(self) -> int:\n",
    "        return self.on_hand + self.on_order\n",
    "\n",
    "\n",
    "InvOrderMapping = Mapping[\n",
    "    InventoryState,\n",
    "    Mapping[int, Categorical[Tuple[InventoryState, float]]]\n",
    "]\n",
    "\n",
    "\n",
    "class SimpleInventoryMDPCap(FiniteMarkovDecisionProcess[InventoryState, int]):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        capacity: int,\n",
    "        poisson_lambda: float,\n",
    "        holding_cost: float,\n",
    "        stockout_cost: float\n",
    "    ):\n",
    "        self.capacity: int = capacity\n",
    "        self.poisson_lambda: float = poisson_lambda\n",
    "        self.holding_cost: float = holding_cost\n",
    "        self.stockout_cost: float = stockout_cost\n",
    "\n",
    "        self.poisson_distr = poisson(poisson_lambda)\n",
    "        super().__init__(self.get_action_transition_reward_map())\n",
    "\n",
    "    def get_action_transition_reward_map(self) -> InvOrderMapping:\n",
    "        d: Dict[InventoryState, Dict[int, Categorical[Tuple[InventoryState,\n",
    "                                                            float]]]] = {}\n",
    "\n",
    "        for alpha in range(self.capacity + 1):\n",
    "            for beta in range(self.capacity + 1 - alpha):\n",
    "                state: InventoryState = InventoryState(alpha, beta)\n",
    "                ip: int = state.inventory_position()\n",
    "                base_reward: float = - self.holding_cost * alpha\n",
    "                d1: Dict[int, Categorical[Tuple[InventoryState, float]]] = {}\n",
    "\n",
    "                for order in range(self.capacity - ip + 1):\n",
    "                    sr_probs_dict: Dict[Tuple[InventoryState, float], float] =\\\n",
    "                        {(InventoryState(ip - i, order), base_reward):\n",
    "                         self.poisson_distr.pmf(i) for i in range(ip)}\n",
    "\n",
    "                    probability: float = 1 - self.poisson_distr.cdf(ip - 1)\n",
    "                    reward: float = base_reward - self.stockout_cost *\\\n",
    "                        (probability * (self.poisson_lambda - ip) +\n",
    "                         ip * self.poisson_distr.pmf(ip))\n",
    "                    sr_probs_dict[(InventoryState(0, order), reward)] = \\\n",
    "                        probability\n",
    "                    d1[order] = Categorical(sr_probs_dict)\n",
    "\n",
    "                d[state] = d1\n",
    "        return d"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "579dcdfb899fc187fdf97538744cc3a387eca9f5084bb8d1591af8d2d48fab3d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
