{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CME 241 Assignment 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shaan Patel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Iterator, TypeVar, Mapping, Callable\n",
    "import rl.markov_decision_process as mp\n",
    "from rl.approximate_dynamic_programming import NTStateDistribution, QValueFunctionApprox\n",
    "from rl.policy import Policy, DeterministicPolicy, RandomPolicy, UniformPolicy\n",
    "from rl.distribution import Categorical\n",
    "from rl.returns import returns\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = TypeVar('S')\n",
    "A = TypeVar('A')\n",
    "\n",
    "def greedy_policy_from_qvf(\n",
    "    q: QValueFunctionApprox[S,A],\n",
    "    actions: Callable[[mp.NonTerminal[S]], Iterable[A]]\n",
    ") -> DeterministicPolicy[S,A]:\n",
    "    def optimal_action(s: S) -> A:\n",
    "        _, a = q.argmax((mp.NonTerminal(s), a) for a in actions(mp.NonTerminal(s)))\n",
    "        return a\n",
    "    return DeterministicPolicy(optimal_action)\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(\n",
    "    q: QValueFunctionApprox[S,A],\n",
    "    mdp: mp.MarkovDecisionProcess[S,A],\n",
    "    epsilon: float = 0.0\n",
    ") -> Policy[S,A]:\n",
    "    def explore(s: S, mdp=mdp) -> Iterable[A]:\n",
    "        return mdp.actions(mp.NonTerminal(s))\n",
    "    return RandomPolicy(Categorical(\n",
    "        {UniformPolicy(explore): epsilon,\n",
    "        greedy_policy_from_qvf(q, mdp.actions): 1 - epsilon}\n",
    "    ))\n",
    "\n",
    "\n",
    "def mc_glie(\n",
    "    mdp: mp.MarkovDecisionProcess[S,A],\n",
    "    states: NTStateDistribution[S],\n",
    "    approx_0: QValueFunctionApprox[S,A],\n",
    "    gamma: float,\n",
    "    epsilon_func: Callable[[int], float],\n",
    "    episode_length_tolerance: float = 1e-6\n",
    ") -> Iterator[QValueFunctionApprox[S,A]]:\n",
    "\n",
    "    num_episodes = 0\n",
    "    q: QValueFunctionApprox[S,A] = approx_0\n",
    "    p: Policy[S,A] = epsilon_greedy_policy(q,mdp, 1.0)\n",
    "    yield q\n",
    "\n",
    "    while True:\n",
    "        trace: Iterable[mp.TransitionStep[S,A]] = mdp.simulate_actions(states,p)\n",
    "        num_episodes += 1\n",
    "        for step in returns(trace, gamma, episode_length_tolerance):\n",
    "            q = q.update([((step.state, step.action), step.return_)])\n",
    "        p = epsilon_greedy_policy(q,mdp,\n",
    "        epsilon_func(num_episodes))\n",
    "        yield q\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabular_gpfq(\n",
    "    q: Mapping[S,Mapping[A, float]],\n",
    "    actions: Callable[[mp.NonTerminal[S]], Iterable[A]]\n",
    ") -> DeterministicPolicy[S,A]:\n",
    "    def optimal_action(s:S) -> A:\n",
    "        max_act = max(q[s], key = q[s].get)\n",
    "        return max_act\n",
    "    return DeterministicPolicy(optimal_action)\n",
    "\n",
    "\n",
    "def tabular_egp(\n",
    "    q: Mapping[S, Mapping[A, float]],\n",
    "    mdp: mp.MarkovDecisionProcess[S,A],\n",
    "    epsilon: float = 0.0\n",
    ") -> Policy[S,A]:\n",
    "    def explore(s: S, mdp=mdp) -> Iterable[A]:\n",
    "        return mdp.actions(mp.NonTerminal(s))\n",
    "    return RandomPolicy(Categorical(\n",
    "        {UniformPolicy(explore): epsilon,\n",
    "        tabular_gpfq(q, mdp.actions): 1 - epsilon}\n",
    "    ))\n",
    "\n",
    "def tabular_mc_glie(\n",
    "    mdp: mp.MarkovDecisionProcess[S,A],\n",
    "    states: NTStateDistribution[S],\n",
    "    approx_0: Mapping[S, Mapping[A, float]],\n",
    "    gamma: float,\n",
    "    episode_length_tolerance: float = 1e-6\n",
    ") -> Iterator[Mapping[S, Mapping[A, float]]]:\n",
    "\n",
    "    num_episodes = 0\n",
    "    counts = defaultdict(lambda: 0)\n",
    "    q: Mapping[S,Mapping[A,float]] = approx_0\n",
    "    p: Policy[S,A] = tabular_egp(q,mdp,1.0)\n",
    "\n",
    "    yield q\n",
    "\n",
    "    while True:\n",
    "        trace: Iterable[mp.TransitionStep[S,A]] = mdp.simulate_actions(states, p)\n",
    "        num_episodes += 1\n",
    "        for step in returns(trace, gamma, episode_length_tolerance):\n",
    "            counts[(step.state, step.action)] += 1\n",
    "            q[step.state][step.action] += (1/counts[(step.state,step.action)])*\\\n",
    "                (step.return_ - q[step.state][step.action])\n",
    "        epsilon = 1/num_episodes\n",
    "        p = tabular_egp(q,mdp,epsilon)\n",
    "        yield q\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from typing import Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action(\n",
    "    q: QValueFunctionApprox[S, A],\n",
    "    nt_state: mp.NonTerminal[S],\n",
    "    actions: Set[A],\n",
    "    epsilon: float\n",
    ") -> A:\n",
    "    greedy_action: A = max(\n",
    "        ((a, q((nt_state, a))) for a in actions),\n",
    "        key=itemgetter(1)\n",
    "    )[0]\n",
    "    return Categorical(\n",
    "        {a: epsilon / len(actions) +\n",
    "        (1 - epsilon if a == greedy_action else 0.) for a in actions}\n",
    "    ).sample()\n",
    "\n",
    "\n",
    "def glie_sarsa(\n",
    "    mdp: mp.MarkovDecisionProcess[S, A],\n",
    "    states: NTStateDistribution[S],\n",
    "    approx_0: QValueFunctionApprox[S, A],\n",
    "    gamma: float,\n",
    "    epsilon_as_func_of_episodes: Callable[[int], float],\n",
    "    max_episode_length: int\n",
    ") -> Iterator[QValueFunctionApprox[S, A]]:\n",
    "    q: QValueFunctionApprox[S, A] = approx_0\n",
    "    yield q\n",
    "    num_episodes: int = 0\n",
    "    while True:\n",
    "        num_episodes += 1\n",
    "        epsilon: float = epsilon_as_func_of_episodes(num_episodes)\n",
    "        state: mp.NonTerminal[S] = states.sample()\n",
    "        action: A = epsilon_greedy_action(\n",
    "            q=q,\n",
    "            nt_state=state,\n",
    "            actions=set(mdp.actions(state)),\n",
    "            epsilon=epsilon\n",
    "        )\n",
    "        steps: int = 0\n",
    "        while isinstance(state, mp.NonTerminal) and steps < max_episode_length:\n",
    "            next_state, reward = mdp.step(state, action).sample()\n",
    "            if isinstance(next_state, mp.NonTerminal):\n",
    "                next_action: A = epsilon_greedy_action(\n",
    "                    q=q,\n",
    "                    nt_state=next_state,\n",
    "                    actions=set(mdp.actions(next_state)),\n",
    "                    epsilon=epsilon\n",
    "                )\n",
    "                q = q.update([(\n",
    "                    (state, action),\n",
    "                    reward + gamma * q((next_state, next_action))\n",
    "                )])\n",
    "                action = next_action\n",
    "            else:\n",
    "                q = q.update([((state, action), reward)])\n",
    "            yield q\n",
    "            steps += 1\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabular_ega(\n",
    "    q: Mapping[S, Mapping[A, float]],\n",
    "    states: mp.NonTerminal[S],\n",
    "    actions: Set[A],\n",
    "    epsilon: float\n",
    ") -> A:\n",
    "    greedy_action: A = max(\n",
    "        (q[states][a] for a in actions),\n",
    "        key = q[states].get\n",
    "    )\n",
    "    return Categorical(\n",
    "        {a: epsilon / len(actions) +\n",
    "        (1 - epsilon if a == greedy_action else 0.) for a in actions}\n",
    "    ).sample()\n",
    "\n",
    "\n",
    "def tabular_sarsa(\n",
    "    mdp: mp.MarkovDecisionProcess[S,A],\n",
    "    states: NTStateDistribution[S],\n",
    "    approx_0: Mapping[S, Mapping[A, float]],\n",
    "    gamma: float,\n",
    "    max_episode_length: int\n",
    ") -> Iterator[Mapping[S,Mapping[A,float]]]:\n",
    "\n",
    "    counts = defaultdict(lambda: 0)\n",
    "    q: Mapping[S,Mapping[A,float]] = approx_0\n",
    "    yield q\n",
    "    num_episodes: int = 0\n",
    "\n",
    "    while True:\n",
    "        num_episodes += 1\n",
    "        epsilon: float = 1/num_episodes\n",
    "        state: mp.NonTerminal[S] = states.sample()\n",
    "        action: A = tabular_ega(\n",
    "            q=q,\n",
    "            states=state,\n",
    "            actions=set(mdp.actions(state)),\n",
    "            epsilon=epsilon\n",
    "        )\n",
    "        steps: int = 0\n",
    "        while isinstance(state,mp.NonTerminal) and steps < max_episode_length:\n",
    "            counts[(state,action)] += 1\n",
    "            next_state, reward = mdp.step(state, action).sample()\n",
    "            if isinstance(next_state, mp.NonTerminal):\n",
    "                next_action: A = tabular_ega(\n",
    "                    q=q,\n",
    "                    states=next_state,\n",
    "                    actions=set(mdp.actions(next_state)),\n",
    "                    epsilon=epsilon\n",
    "                )\n",
    "                q[state][action] += (1/counts[(state,action)])*\\\n",
    "                    (reward + gamma*q[next_state][next_action] - q[state][action])\n",
    "                action = next_action\n",
    "            else:\n",
    "                q[state][action] += (1/counts[(state,action)])*reward\n",
    "            yield q\n",
    "            steps += 1\n",
    "            state = next_state"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "579dcdfb899fc187fdf97538744cc3a387eca9f5084bb8d1591af8d2d48fab3d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
