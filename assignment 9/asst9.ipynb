{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CME 241 Assignment 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shaan Patel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "from typing import Dict, Tuple\n",
    "from rl.chapter9.order_book import DollarsAndShares, PriceSizePairs, OrderBook\n",
    "from rl.distribution import Distribution, Categorical\n",
    "from rl.markov_process import MarkovProcess, NonTerminal, State\n",
    "from numpy.random import poisson, random_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimitOrder(Distribution[OrderBook]):\n",
    "    def __init__(self, book: OrderBook):\n",
    "        self.book = book\n",
    "    \n",
    "    @abstractmethod\n",
    "    def sample(self):\n",
    "        x = random_sample()\n",
    "        num_shares = poisson(1)\n",
    "        if x > 0.5:\n",
    "            price = self.book.ask_price()*random_sample()\n",
    "            ret_tuple = self.book.sell_limit_order(price, num_shares)\n",
    "            return ret_tuple[1]\n",
    "        else:\n",
    "            price = 10*random_sample() + self.book.bid_price()\n",
    "            ret_tuple = self.book.buy_limit_order(price, num_shares)\n",
    "            return ret_tuple[1]\n",
    "\n",
    "class MarketOrder(Distribution[OrderBook]):\n",
    "    def __init__(self, book: OrderBook):\n",
    "        self.book = book\n",
    "\n",
    "    def sample(self):\n",
    "        x = random_sample()\n",
    "        num_shares = poisson(1)\n",
    "        if x > 0.5:\n",
    "            ret_tuple = self.book.sell_market_order(num_shares)\n",
    "            return ret_tuple[1]\n",
    "        else:\n",
    "            ret_tuple = self.book.buy_market_order(num_shares)\n",
    "            return ret_tuple[1]\n",
    "\n",
    "\n",
    "\n",
    "class OBDynam(MarkovProcess[OrderBook]):\n",
    "    def transition(self, state: OrderBook) -> Distribution[OrderBook]:\n",
    "        x = random_sample()\n",
    "        if x > 0.5:\n",
    "            LO = LimitOrder[state]\n",
    "            return LO\n",
    "        else:\n",
    "            MO = MarketOrder[state]\n",
    "            return MO\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bids: PriceSizePairs = [DollarsAndShares(\n",
    "        dollars=x,\n",
    "        shares=poisson(100. - (100 - x) * 10)\n",
    "    ) for x in range(100, 90, -1)]\n",
    "asks: PriceSizePairs = [DollarsAndShares(\n",
    "        dollars=x,\n",
    "        shares=poisson(100. - (x - 105) * 10)\n",
    "    ) for x in range(105, 115, 1)]\n",
    "\n",
    "ob0: OrderBook = OrderBook(descending_bids=bids, ascending_asks=asks)\n",
    "\n",
    "dynam = OBDynam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we have a temporary price impact of:\n",
    "\n",
    "$$Q_t = P_t (1- \\beta N_t - \\theta X_t) $$\n",
    "\n",
    "The optimal value function is:\n",
    "\n",
    "$$V^*_t((P_t, R_t)) = \\max_{N_t} \\{N_t P_t(1 - \\beta N_t - \\theta X_t) + E[V^*_{t+1}((P_{t+1},R_{t+1}))]\\} $$\n",
    "\n",
    "$$V^*_{T-1}((P_{T-1}, R_{T-1})) = N_{T-1} P_{T-1}(1 - \\beta N_{T-1} - \\theta X_{T-1})$$\n",
    "\n",
    "$$ = R_{T-1} P_{T-1}(1 - \\beta R_{T-1} - \\theta X_{T-1}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we can infer $V^*_{T-2}((P_{T-2}, R_{T-2}))$ as:\n",
    "\n",
    "$$\\max_{N_{T-2}}\\{N_{T-2}P_{T-2}(1 - \\beta N_{T-2} - \\theta X_{T-2}) + E[R_{T-1} P_{T-1}(1 - \\beta R_{T-1} - \\theta X_{T-1})]\\} $$\n",
    "\n",
    "$$\\max_{N_{T-2}}\\{N_{T-2}P_{T-2}(1 - \\beta N_{T-2} - \\theta X_{T-2}) $$\n",
    "$$+ E[(R_{T-2} - N_{T-2}) P_{T-1}(1 - \\beta (R_{T-2} - N_{T-2}) - \\theta X_{T-1})]\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leads to:\n",
    "\n",
    "$$ \\max_{N_{T-2}}N_{T-2}P_{T-2}(1 - \\beta N_{T-2} - \\theta X_{T-2}) \\\\\n",
    "+ E[(R_{T-2} - N_{T-2}) (P_{T-2}e^{Z_{T-2}})(1 - \\beta (R_{T-2} - N_{T-2}) - \\theta (\\rho X_{T-2} + \\eta_{T-2}))] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected value is removed because we know the means of our random variables. By grouping like terms, we get:\n",
    "\n",
    "$$ \\max_{N_{T_2}} N_{T-2}P_{T-2}(1 - e^{\\mu_Z + \\frac{\\sigma_Z^2}{2}}) - \\beta N^2_{T-2}P_{T-2} (1 + e^{\\mu_Z + \\frac{\\sigma_Z^2}{2}}) \\\\\n",
    " - \\beta P_{T-2}e^{\\mu_Z + \\frac{\\sigma_Z^2}{2}} (R^2_{T-2} - 2N_{T-2}R_{T-2}) \\\\\n",
    "  - \\theta N_{T-2}P_{T-2}X_{T-2}(1 - \\rho e^{\\mu_Z + \\frac{\\sigma_Z^2}{2}}) - \\theta \\rho  R_{T-2} P_{T-2} X_{T-2} e^{\\mu_Z + \\frac{\\sigma_Z^2}{2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the derivative with respect to $N_{T-2}$, we get:\n",
    "\n",
    "$$ P_{T-2}(1 - e^{\\mu_Z + \\frac{\\sigma_Z^2}{2}}) - 2\\beta N_{T-2}P_{T-2}(1 + e^{\\mu_Z + \\frac{\\sigma_Z^2}{2}}) \\\\\n",
    "+ 2\\beta P_{T-2}R_{T-2}e^{\\mu_Z + \\frac{\\sigma_Z^2}{2}} \\\\\n",
    " - \\theta P_{T-2}X_{T-2}(1 - \\rho e^{\\mu_Z + \\frac{\\sigma_Z^2}{2}}) = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\Rarr N^*_{T-2} = \\frac{1 - e^{\\mu_Z + \\frac{\\sigma_Z^2}{2}}}{2\\beta(1 + e^{\\mu_Z + \\frac{\\sigma_Z^2}{2}})} + \\frac{R_{T-2}e^{\\mu_Z + \\frac{\\sigma_Z^2}{2}}}{1 + e^{\\mu_Z + \\frac{\\sigma_Z^2}{2}}} - \\frac{\\theta X_{T-2}(1 - \\rho e^{\\mu_Z + \\frac{\\sigma_Z^2}{2}})}{2\\beta(1 + e^{\\mu_Z + \\frac{\\sigma_Z^2}{2}})} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the constant term and coefficients to $ c^{(1)}_{T-2}$, $c^{(2)}_{T-2}$, and $c^{(3)}_{T-2}$ respectively, we get:\n",
    "\n",
    "$$ N^*_{T-2} =  c^{(1)}_{T-2} + c^{(2)}_{T-2}R_{T-2} + c^{(3)}_{T-2}X_{T-2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the Value Function becomes:\n",
    "\n",
    "$$ V((P_{T-2},R_{T-2},X_{T-2})) = P_{T-2}e^{\\mu_Z + \\frac{\\sigma^2_Z}{2}} (N^*_{T-2}(e^{-\\mu_Z - \\frac{\\sigma^2_Z}{2}} - 1) - \\beta N^*_{T-2}(e^{-\\mu_Z - \\frac{\\sigma^2_Z}{2}} + 1) \\\\\n",
    "- \\beta R^2_{T-2} - 2N^*_{T-2}R_{T-2} - \\theta N^*_{T-2} X_{T-2}(e^{-\\mu_Z - \\frac{\\sigma^2_Z}{2}} - \\rho) - \\theta \\rho R_{T-2}X_{T-2}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will denote $(e^{-\\mu_Z - \\frac{\\sigma^2_Z}{2}} - 1) $ as $\\alpha$, $(e^{-\\mu_Z - \\frac{\\sigma^2_Z}{2}} + 1) $ as $ \\gamma$, and $(e^{-\\mu_Z - \\frac{\\sigma^2_Z}{2}} - \\rho) $ as $v$. This gives us:\n",
    "\n",
    "$$ V_{T-2} = P_{T-2}e^{\\mu_Z + \\frac{\\sigma^2_Z}{2}}(\\alpha N^*_{T-2} - \\beta \\gamma N^*_{T-2} - \\beta R^2_{T-2} - 2N^*_{T-2}R_{T-2} \\\\\n",
    " - \\theta v N^*_{T-2} X_{T-2} - \\theta \\rho R_{T-2} X_{T-2}) $$\n",
    "\n",
    " $$\\Rarr P_{T-2}e^{\\mu_Z + \\frac{\\sigma^2_Z}{2}}(N^*_{T-2}(\\alpha - \\beta \\gamma - 2 R_{T-2} - \\theta v X_{T-2}) - \\beta R^2_{T-2} - \\theta \\rho R_{T-2}X_{T-2}) $$\n",
    "\n",
    " $$\\Rarr  P_{T-2}e^{\\mu_Z + \\frac{\\sigma^2_Z}{2}} ((c^{(1)}_{T-2} + c^{(2)}_{T-2}R_{T-2} + c^{(3)}_{T-2}X_{T-2})(\\alpha - \\beta \\gamma - 2 R_{T-2} - \\theta v X_{T-2}) \\\\\n",
    " - \\beta R^2_{T-2} - \\theta \\rho R_{T-2}X_{T-2}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When grouping terms we can assign the coefficients as constants:\n",
    "\n",
    "$$\\Rarr P_{T-2}e^{\\mu_Z + \\frac{\\sigma^2_Z}{2}}(c^{(4)}_{T-2} + c^{(5)}_{T-2} R_{T-2} + c^{(6)}_{T-2} X_{T-2} + c^{(7)}_{T-2}R^2_{T-2} \\\\\n",
    " + c^{(8)}_{T-2} X^2_{T-2} + c^{(9)}_{T-2} R_{T-2} X_{T-2}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing backwards in time, we see that this generalized solution is the same for all $t$:\n",
    "\n",
    "$$ N^*_t =  c^{(1)}_t + c^{(2)}_t R_t + c^{(3)}_t X_t$$\n",
    "\n",
    "$$ V^*_t = P_t e^{\\mu_Z + \\frac{\\sigma^2_Z}{2}}(c^{(4)}_t + c^{(5)}_t R_t + c^{(6)}_t X_t + c^{(7)}_t R^2_t \\\\\n",
    " + c^{(8)}_t X^2_t + c^{(9)}_t R_t X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.chapter9.optimal_order_execution import OptimalOrderExecution, PriceAndShares\n",
    "from rl.function_approx import FunctionApprox, LinearFunctionApprox\n",
    "from rl.distribution import Gaussian, SampledDistribution\n",
    "from rl.policy import DeterministicPolicy\n",
    "from rl.approximate_dynamic_programming import ValueFunctionApprox\n",
    "from typing import Tuple, Iterator, Sequence, Callable\n",
    "from numpy.random import standard_normal\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriceSharesX:\n",
    "    price: float\n",
    "    shares: int\n",
    "    x: float\n",
    "\n",
    "class LPT(OptimalOrderExecution):\n",
    "    shares: int\n",
    "    time_steps: int\n",
    "    avg_exec_price_diff: Sequence[Callable[[PriceSharesX], float]]\n",
    "    price_dynamics: Sequence[Callable[[PriceSharesX], Distribution[float]]]\n",
    "    x_dynamics: Sequence[Callable[[PriceSharesX], Distribution[float]]]\n",
    "    utility_func: Callable[[float], float]\n",
    "    discount_factor: float\n",
    "    func_approx: ValueFunctionApprox[PriceAndShares]\n",
    "    initial_price_distribution: Distribution[float]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24496/3290925740.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m it_vf: Iterator[Tuple[ValueFunctionApprox[PriceAndShares],\n\u001b[0;32m     45\u001b[0m                         DeterministicPolicy[PriceAndShares, int]]] = \\\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mooe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward_induction_vf_and_pi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m state: PriceAndShares = PriceAndShares(\n",
      "\u001b[1;32mc:\\users\\shaan\\onedrive\\desktop\\stanford\\senior winter\\reinforcement learning\\rl-book\\rl\\chapter9\\optimal_order_execution.py\u001b[0m in \u001b[0;36mbackward_induction_vf_and_pi\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[0merror_tolerance\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         return back_opt_vf_and_policy(\n\u001b[0m\u001b[0;32m    151\u001b[0m             \u001b[0mmdp_f0_mu_triples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmdp_f0_mu_triples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[0mγ\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscount_factor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shaan\\onedrive\\desktop\\stanford\\senior winter\\reinforcement learning\\rl-book\\rl\\approximate_dynamic_programming.py\u001b[0m in \u001b[0;36mback_opt_vf_and_policy\u001b[1;34m(mdp_f0_mu_triples, γ, num_state_samples, error_tolerance)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m         this_v = approx0.solve(\n\u001b[1;32m--> 266\u001b[1;33m             [(s, max(mdp.step(s, a).expectation(return_)\n\u001b[0m\u001b[0;32m    267\u001b[0m                      for a in mdp.actions(s)))\n\u001b[0;32m    268\u001b[0m              for s in mu.sample_n(num_state_samples)],\n",
      "\u001b[1;32mc:\\users\\shaan\\onedrive\\desktop\\stanford\\senior winter\\reinforcement learning\\rl-book\\rl\\approximate_dynamic_programming.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m         this_v = approx0.solve(\n\u001b[1;32m--> 266\u001b[1;33m             [(s, max(mdp.step(s, a).expectation(return_)\n\u001b[0m\u001b[0;32m    267\u001b[0m                      for a in mdp.actions(s)))\n\u001b[0;32m    268\u001b[0m              for s in mu.sample_n(num_state_samples)],\n",
      "\u001b[1;32mc:\\users\\shaan\\onedrive\\desktop\\stanford\\senior winter\\reinforcement learning\\rl-book\\rl\\approximate_dynamic_programming.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m         this_v = approx0.solve(\n\u001b[1;32m--> 266\u001b[1;33m             [(s, max(mdp.step(s, a).expectation(return_)\n\u001b[0m\u001b[0;32m    267\u001b[0m                      for a in mdp.actions(s)))\n\u001b[0;32m    268\u001b[0m              for s in mu.sample_n(num_state_samples)],\n",
      "\u001b[1;32mc:\\users\\shaan\\onedrive\\desktop\\stanford\\senior winter\\reinforcement learning\\rl-book\\rl\\distribution.py\u001b[0m in \u001b[0;36mexpectation\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         '''\n\u001b[1;32m---> 92\u001b[1;33m         return sum(f(self.sample()) for _ in\n\u001b[0m\u001b[0;32m     93\u001b[0m                    range(self.expectation_samples)) / self.expectation_samples\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shaan\\onedrive\\desktop\\stanford\\senior winter\\reinforcement learning\\rl-book\\rl\\distribution.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         '''\n\u001b[1;32m---> 92\u001b[1;33m         return sum(f(self.sample()) for _ in\n\u001b[0m\u001b[0;32m     93\u001b[0m                    range(self.expectation_samples)) / self.expectation_samples\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shaan\\onedrive\\desktop\\stanford\\senior winter\\reinforcement learning\\rl-book\\rl\\approximate_dynamic_programming.py\u001b[0m in \u001b[0;36mreturn_\u001b[1;34m(s_r, i)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mreturn_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms_r\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mState\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             \u001b[0ms1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms_r\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mγ\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mextended_vf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m         this_v = approx0.solve(\n",
      "\u001b[1;32mc:\\users\\shaan\\onedrive\\desktop\\stanford\\senior winter\\reinforcement learning\\rl-book\\rl\\approximate_dynamic_programming.py\u001b[0m in \u001b[0;36mextended_vf\u001b[1;34m(vf, s)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mextended_vf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mValueFunctionApprox\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mState\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_non_terminal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shaan\\onedrive\\desktop\\stanford\\senior winter\\reinforcement learning\\rl-book\\rl\\markov_process.py\u001b[0m in \u001b[0;36mon_non_terminal\u001b[1;34m(self, f, default)\u001b[0m\n\u001b[0;32m     26\u001b[0m     ) -> X:\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNonTerminal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shaan\\onedrive\\desktop\\stanford\\senior winter\\reinforcement learning\\rl-book\\rl\\function_approx.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x_value)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_value\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx_value\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shaan\\onedrive\\desktop\\stanford\\senior winter\\reinforcement learning\\rl-book\\rl\\function_approx.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x_values_seq)\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_values_seq\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m         return np.dot(\n\u001b[0m\u001b[0;32m    593\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_values_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "init_price_mean: float = 100.0\n",
    "init_price_stdev: float = 10.0\n",
    "num_shares: int = 100\n",
    "num_time_steps: int = 5\n",
    "beta: float = 0.05\n",
    "theta: float = 0.05\n",
    "rho: float = 0.05\n",
    "\n",
    "x = []\n",
    "x.append(1)\n",
    "for i in range(1,num_time_steps):\n",
    "    x.append(rho*x[i-1] + standard_normal())\n",
    "\n",
    "stand_norm = Gaussian(0,1)\n",
    "\n",
    "class ExpGauss(SampledDistribution[float]):\n",
    "    def __init__(self, mu, expectation_samples: int = 10000):\n",
    "        self.mu = mu\n",
    "        super().__init__(\n",
    "            sampler=lambda: mu*exp(standard_normal()),\n",
    "            expectation_samples=expectation_samples\n",
    "        )\n",
    "\n",
    "\n",
    "price_diff = [lambda p_s: beta*p_s.shares + x[i] for _ in range(num_time_steps)]\n",
    "dynamics = [lambda p_s: ExpGauss(p_s.price) for _ in range(num_time_steps)]\n",
    "ffs = [\n",
    "    lambda p_s: p_s.state.price * p_s.state.shares,\n",
    "    lambda p_s: float(p_s.state.shares * p_s.state.shares)\n",
    "]\n",
    "fa: FunctionApprox = LinearFunctionApprox.create(feature_functions=ffs)\n",
    "init_price_distrib: Gaussian = Gaussian(init_price_mean, init_price_stdev)\n",
    "\n",
    "ooe: OptimalOrderExecution = OptimalOrderExecution(\n",
    "    shares=num_shares,\n",
    "    time_steps=num_time_steps,\n",
    "    avg_exec_price_diff=price_diff,\n",
    "    price_dynamics=dynamics,\n",
    "    utility_func=lambda x: x,\n",
    "    discount_factor=1,\n",
    "    func_approx=fa,\n",
    "    initial_price_distribution=init_price_distrib\n",
    ")\n",
    "it_vf: Iterator[Tuple[ValueFunctionApprox[PriceAndShares],\n",
    "                        DeterministicPolicy[PriceAndShares, int]]] = \\\n",
    "    ooe.backward_induction_vf_and_pi()\n",
    "\n",
    "state: PriceAndShares = PriceAndShares(\n",
    "    price=init_price_mean,\n",
    "    shares=num_shares\n",
    ")\n",
    "print(\"Backward Induction: VF And Policy\")\n",
    "print(\"---------------------------------\")\n",
    "print()\n",
    "for t, (vf, pol) in enumerate(it_vf):\n",
    "    print(f\"Time {t:d}\")\n",
    "    print()\n",
    "    opt_sale: int = pol.action_for(state)\n",
    "    val: float = vf(NonTerminal(state))\n",
    "    print(f\"Optimal Sales = {opt_sale:d}, Opt Val = {val:.3f}\")\n",
    "    print()\n",
    "    print(\"Optimal Weights below:\")\n",
    "    print(vf.weights.weights)\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "579dcdfb899fc187fdf97538744cc3a387eca9f5084bb8d1591af8d2d48fab3d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
